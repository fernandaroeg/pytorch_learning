{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2cvA6soQQFV"
   },
   "source": [
    "# MLP with Single Hidden Layer using PyTorch\n",
    "\n",
    "1. Define an MLP with variable number of inputs (num_inputs), outputs (num_outputs), and nodes in hidden layer (num_hidden_layer_nodes).  \n",
    "2. Use ReLU activation for each node \n",
    "3. Use MSE loss\n",
    "4. Use SGD optimizer\n",
    "\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/mlp.png\" alt=\"mlp\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PPyTXeGJQcuY",
    "outputId": "81d37dac-ed1a-4cd6-c51f-0be62d487340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10581.46484375\n",
      "1 9755.71875\n",
      "2 9161.2998046875\n",
      "3 8637.4951171875\n",
      "4 8135.08154296875\n",
      "5 7634.916015625\n",
      "6 7127.3212890625\n",
      "7 6610.970703125\n",
      "8 6087.37255859375\n",
      "9 5563.39599609375\n",
      "10 5048.94873046875\n",
      "11 4551.427734375\n",
      "12 4078.5126953125\n",
      "13 3634.7451171875\n",
      "14 3225.676025390625\n",
      "15 2852.951904296875\n",
      "16 2516.31201171875\n",
      "17 2214.470947265625\n",
      "18 1946.52880859375\n",
      "19 1709.119873046875\n",
      "20 1499.035400390625\n",
      "21 1313.6361083984375\n",
      "22 1151.3741455078125\n",
      "23 1009.0541381835938\n",
      "24 883.8927001953125\n",
      "25 774.8145141601562\n",
      "26 679.2298583984375\n",
      "27 596.1323852539062\n",
      "28 524.0944213867188\n",
      "29 462.283935546875\n",
      "30 411.1572570800781\n",
      "31 372.4652099609375\n",
      "32 353.1527404785156\n",
      "33 370.260498046875\n",
      "34 465.5327453613281\n",
      "35 733.2030639648438\n",
      "36 1382.11962890625\n",
      "37 2739.392578125\n",
      "38 5025.6025390625\n",
      "39 6997.73095703125\n",
      "40 6102.787109375\n",
      "41 2681.7080078125\n",
      "42 893.98876953125\n",
      "43 422.0854187011719\n",
      "44 282.61419677734375\n",
      "45 214.14593505859375\n",
      "46 170.4607391357422\n",
      "47 139.28244018554688\n",
      "48 115.85263061523438\n",
      "49 97.65865325927734\n",
      "50 83.18476104736328\n",
      "51 71.4903793334961\n",
      "52 61.886985778808594\n",
      "53 53.91327667236328\n",
      "54 47.2055549621582\n",
      "55 41.530059814453125\n",
      "56 36.68571472167969\n",
      "57 32.512054443359375\n",
      "58 28.9075984954834\n",
      "59 25.780160903930664\n",
      "60 23.049795150756836\n",
      "61 20.657798767089844\n",
      "62 18.55286407470703\n",
      "63 16.698423385620117\n",
      "64 15.059986114501953\n",
      "65 13.60486888885498\n",
      "66 12.310081481933594\n",
      "67 11.154321670532227\n",
      "68 10.12430191040039\n",
      "69 9.204663276672363\n",
      "70 8.380002975463867\n",
      "71 7.6393609046936035\n",
      "72 6.973056793212891\n",
      "73 6.375362396240234\n",
      "74 5.834918975830078\n",
      "75 5.3463215827941895\n",
      "76 4.904284477233887\n",
      "77 4.502889156341553\n",
      "78 4.139032363891602\n",
      "79 3.80806565284729\n",
      "80 3.507455587387085\n",
      "81 3.2341248989105225\n",
      "82 2.9859635829925537\n",
      "83 2.758362293243408\n",
      "84 2.5514285564422607\n",
      "85 2.361090660095215\n",
      "86 2.1866073608398438\n",
      "87 2.0271787643432617\n",
      "88 1.8809770345687866\n",
      "89 1.746887445449829\n",
      "90 1.623244047164917\n",
      "91 1.5095915794372559\n",
      "92 1.4051040410995483\n",
      "93 1.3087643384933472\n",
      "94 1.219848394393921\n",
      "95 1.1378856897354126\n",
      "96 1.0623600482940674\n",
      "97 0.9924880266189575\n",
      "98 0.9276551008224487\n",
      "99 0.8675888180732727\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Get reproducible results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden_layer_nodes, num_outputs):\n",
    "        # Initialize super class\n",
    "        super().__init__() #this class inherits from nn.Module, so we call the constructor of nn.Module \n",
    "                           #(the superclass/father) using super() to use its constructor\n",
    "\n",
    "        # Add hidden layer \n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden_layer_nodes)\n",
    "\n",
    "        # Add output layer\n",
    "        self.linear2 = nn.Linear(num_hidden_layer_nodes, num_outputs)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through hidden layer with \n",
    "        x = F.relu(self.linear1(x))\n",
    "        \n",
    "        # Foward pass to output layer\n",
    "        return self.linear2(x)\n",
    "\n",
    "# Num data points\n",
    "num_data = 1000\n",
    "\n",
    "# Network parameters\n",
    "num_inputs = 1000\n",
    "num_hidden_layer_nodes = 100\n",
    "num_outputs = 10\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100 \n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(num_data, num_inputs)\n",
    "y = torch.randn(num_data, num_outputs)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = MLP(num_inputs, num_hidden_layer_nodes, num_outputs)\n",
    "\n",
    "# Define loss function\n",
    "loss_function = nn.MSELoss(reduction='sum') # reduction='sum' means we are not using the mean, we are simplyfing to SE\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4) # lr = 0.0001\n",
    "\n",
    "\n",
    "for t in range(num_epochs):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_function(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Calculate gradient using backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update model parameters (weights)\n",
    "    optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WyjPulcDSJQo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MLP-without-sequential",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "opencv-env",
   "language": "python",
   "name": "opencv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
